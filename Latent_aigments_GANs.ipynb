{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopie van Latent_Augmentations_in_GANS.ipynb","provenance":[{"file_id":"1359G7pkC5thb-X785Ex0e_058SWAuFmh","timestamp":1621632406325}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP2Y1li7aFEiKTrpV9ALpFd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1JrgilPViFbv"},"source":["## **Code for latent augmentation in GAN training**\n","\n","Code largely adapted from official pytorch DCGAN tutorial:\n","https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"HbXL6A2Whazf"},"source":["**Make sure GPU is on. Change Runtime in toolbar above if necessary**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nXusiMebfVSD","executionInfo":{"status":"ok","timestamp":1621632354722,"user_tz":-120,"elapsed":311,"user":{"displayName":"Stan van der Vossen","photoUrl":"","userId":"04798649917126628037"}},"outputId":"53e54673-4ae9-4664-fb13-b7c3ba31941e"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Fri May 21 21:25:54 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MREs_-_Ghm1G"},"source":["**Download dataset**\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1faSb1hSfYdN","executionInfo":{"status":"ok","timestamp":1621632375690,"user_tz":-120,"elapsed":19795,"user":{"displayName":"Stan van der Vossen","photoUrl":"","userId":"04798649917126628037"}},"outputId":"025fcd07-052e-405c-9f48-2edd78536915"},"source":["!gdown https://drive.google.com/uc?id=1BLs6H4j8cZmFXSK5ID3jjFHYP2GeqcnJ # 3K image dataset used in paper\n","!cp TrainingFaces.zip .\n","!unzip -q TrainingFaces.zip\n","!rm TrainingFaces.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1BLs6H4j8cZmFXSK5ID3jjFHYP2GeqcnJ\n","To: /content/TrainingFaces.zip\n","90.3MB [00:00, 118MB/s] \n","cp: 'TrainingFaces.zip' and './TrainingFaces.zip' are the same file\n","replace fewfaces/67000/67000.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9DQwqTS7hz09"},"source":["**Hyperparameters and helper functions**"]},{"cell_type":"code","metadata":{"id":"KYkcFjT_gnQO","executionInfo":{"status":"ok","timestamp":1621632376394,"user_tz":-120,"elapsed":709,"user":{"displayName":"Stan van der Vossen","photoUrl":"","userId":"04798649917126628037"}}},"source":["from __future__ import print_function\n","#%matplotlib inline\n","import argparse\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","\n","\n","# Number of workers for dataloader\n","workers = 0\n","\n","# Batch size during training\n","batch_size = 128\n","\n","# Number of channels in the training images. For color images this is 3\n","nc = 3\n","\n","# Size of z latent vector to input to the generator\n","nz = 100\n","\n","# Size of feature maps in generator\n","ngf = 64\n","# Size of feature maps in discriminator\n","ndf = 64\n","\n","# Number of training epochs\n","num_epochs = 400\n","\n","# Learning rate for optimizers\n","lr = 0.0001\n","\n","# Beta1 hyperparam for Adam optimizers\n","beta1 = 0.5\n","\n","# Number of GPUs available. Use 0 for CPU mode.\n","ngpu = 1\n","\n","# We can use an image folder dataset the way we have it setup.\n","# Create the dataset\n","dataset = dset.ImageFolder(root=\"fewfaces\",\n","                           transform=transforms.Compose([\n","                               transforms.Resize(64),\n","                               transforms.CenterCrop(64), # Images most be resized to 64x64 to fir the networks\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                           ]))\n","# Create the dataloader\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n","                                         shuffle=True, num_workers=0)\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n","\n","def weights_init(m):\n","    # Initialize weights of networks as normally distributed\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        nn.init.normal_(m.weight.data, 0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        nn.init.constant_(m.bias.data, 0)\n","\n","def PCA_torch(X,k, center=True, scale=False):\n","    # Calculate principal components using pytorch\n","    # Adapted from: https://medium.com/@ravikalia/pca-done-from-scratch-with-python-2b5eb2790bfc\n","    n,p = X.size()\n","    ones = torch.ones(n, device=device).view([n,1])\n","    h = ((1/n) * torch.mm(ones, ones.t())) if center  else torch.zeros(n*n).view([n,n])\n","    H = torch.eye(n, device=device) - h\n","    X_center =  torch.mm(H.double(), X.double())\n","    covariance = 1/(n-1) * torch.mm(X_center.t(), X_center).view(p,p)\n","    scaling =  torch.sqrt(1/torch.diag(covariance)).double() if scale else torch.ones(p,device=device).double()\n","    scaled_covariance = torch.mm(torch.diag(scaling).view(p,p), covariance)\n","    eigenvalues, eigenvectors = torch.eig(scaled_covariance, True)\n","    components = (eigenvectors[:, :k]).t()\n","    explained_variance = eigenvalues[:k, 0]\n","    return {'components':components,     \n","       'explained_variance':explained_variance }"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Do1SVqZTi8qv"},"source":["**Create and initialize Generator and Discriminator networks**"]},{"cell_type":"code","metadata":{"id":"OnxuyyTghQgO","executionInfo":{"status":"ok","timestamp":1621632380008,"user_tz":-120,"elapsed":3616,"user":{"displayName":"Stan van der Vossen","photoUrl":"","userId":"04798649917126628037"}}},"source":["class Generator(nn.Module):\n","    def __init__(self, ngpu):\n","        super(Generator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            # input is Z, going into a convolution\n","            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            # state size. (ngf*8) x 4 x 4\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            # state size. (ngf*4) x 8 x 8\n","            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            # state size. (ngf*2) x 16 x 16\n","            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            # state size. (ngf) x 32 x 32\n","            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n","            nn.Tanh()\n","            # state size. (nc) x 64 x 64\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)\n","\n","netG = Generator(ngpu).to(device)\n","netG.apply(weights_init)\n","\n","class Discriminator(nn.Module):\n","   def __init__(self, ngpu, augtensor=0, augment_strength=0.15, \n","                augment_probability=0.15,directions = 0):\n","       super(Discriminator, self).__init__()\n","       self.ngpu = ngpu\n","       self.augment_strength = augment_strength \n","       self.augment_probability = augment_probability * 2\n","       self.augtensor = augtensor\n","       self.directions = directions\n","\n","       self.main = nn.Sequential(\n","           # input is (nc) x 64 x 64\n","           nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n","           nn.LeakyReLU(0.2),\n","           # state size. (ndf) x 32 x 32\n","           nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n","           nn.BatchNorm2d(ndf * 2),\n","           nn.LeakyReLU(0.2),\n","           # state size. (ndf*2) x 16 x 16\n","           nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n","           nn.BatchNorm2d(ndf * 4),\n","           nn.LeakyReLU(0.2),\n","           # state size. (ndf*4) x 8 x 8\n","           nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n","           nn.BatchNorm2d(ndf * 8),\n","           nn.LeakyReLU(0.2),\n","           # state size. (ndf*8) x 4 x 4\n","           nn.Conv2d(ndf * 8, 64, 4, 1, 0, bias=False),\n","           nn.LeakyReLU(0.2),\n","           nn.Flatten(),\n","\n","           nn.Linear(64,16),\n","           nn.LeakyReLU(0.2),\n","           )\n","       self.final = nn.Sequential(\n","           nn.Linear(16,8),\n","           nn.LeakyReLU(0.2),\n","           nn.Linear(8,1),\n","           nn.Sigmoid()\n","       )\n","   def set_augmentstrength(self, strength):\n","      # Fucntion to dynamically change the strength of augmentations\n","      self.aug_strength = strength\n","   def set_augmentprob(self, probability):\n","      # Fucntion to dynamically change the probability of augmentations\n","      self.augment_probability = probability\n","   def encode(self, real_ims):    \n","      # encode data to \n","      self.augtensor = self.main(real_ims).detach()\n","   def calculate_covariance(self):\n","      temp = PCA_torch(self.augtensor,4) # Calculate the 4 biggerst components\n","      directions = temp[\"components\"]\n","      variance = temp[\"explained_variance\"].reshape(-1,1)\n","\n","      scaled_directions = torch.mul(directions, variance)\n","      self.directions = scaled_directions.detach()\n","   def forward(self, input):\n","       augment_probability = min(self.augment_probability,1) \n","       augment_strength = self.augment_strength + 0.00001\n","       aug = self.directions\n","       x = self.main(input)\n","       \n","       empty = torch.zeros(x.shape[0],aug.shape[0],dtype=torch.float32)\n","       init_sparse = torch.empty_like(empty,dtype=torch.float32, device=device).uniform_(0, augment_probability)\n","       sparse_matrix = torch.bernoulli(init_sparse)\n","       scaled_directions = self.directions.detach()\n","       augmentations = torch.matmul(sparse_matrix, scaled_directions.float()).detach()\n","\n","       x += augmentations\n","       x = self.final(x) \n","       return x\n","netD = Discriminator(ngpu).to(device)\n","netD.apply(weights_init)\n","netG.apply(weights_init)\n","netD.encode(next(iter(dataloader))[0].to(device))\n","netD.calculate_covariance()"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0ci2oDfjEj3"},"source":["**Training loop**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272},"id":"XvCrHmg-hVlf","executionInfo":{"status":"error","timestamp":1621632386698,"user_tz":-120,"elapsed":6701,"user":{"displayName":"Stan van der Vossen","photoUrl":"","userId":"04798649917126628037"}},"outputId":"e743c662-f6d0-4f40-a7a1-f2db113f12a1"},"source":["criterion = nn.BCELoss()\n","\n","# Create batch of latent vectors that we will use to visualize\n","#  the progression of the generator\n","fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n","\n","# Establish convention for real and fake labels during training\n","real_label = 1.\n","fake_label = 0.\n","\n","# Setup Adam optimizers for both G and D\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n","\n","# Training Loop\n","\n","# Lists to keep track of progress\n","img_list = []\n","G_losses = []\n","D_losses = []\n","iters = 0\n","images = 0 # Number of images generated\n","\n","print(\"Starting Training Loop...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    \n","    if images >= 100000:\n","      # Once threshhold images is reached, perform latent augmenations\n","\n","      # Apply augmentation strength and probability\n","      netD.set_augmentstrength(0.15)\n","      netD.augmentprob(0.1)\n","      \n","      # Calculate directions of covariance onze every epoch\n","      netD.encode(next(iter(dataloader))[0].to(device)) # encode directions of covariance\n","      netD.calculate_covariance()\n","\n","    for i, data in enumerate(dataloader, 0):\n","        ## Train with all-real batch\n","        \n","        netD.zero_grad()\n","        # Format batch\n","        real_cpu = data[0].to(device)\n","        b_size = real_cpu.size(0)\n","        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n","        # Forward pass real batch through D\n","\n","        output = netD(real_cpu).view(-1)\n","        # Calculate loss on all-real batch\n","        errD_real = criterion(output, label)\n","        # Calculate gradients for D in backward pass\n","        errD_real.backward()\n","        D_x = output.mean().item()\n","\n","        ## Train with all-fake batch\n","        # Generate batch of latent vectors\n","        noise = torch.randn(b_size, nz, 1, 1, device=device)\n","        # Generate fake image batch with G\n","        fake = netG(noise)\n","        label.fill_(fake_label)\n","        # Classify all fake batch with D\n","        output = netD(fake.detach()).view(-1)\n","        # Calculate D's loss on the all-fake batch\n","        errD_fake = criterion(output, label)\n","        # Calculate the gradients for this batch\n","        errD_fake.backward()\n","        D_G_z1 = output.mean().item()\n","        # Add the gradients from the all-real and all-fake batches\n","        errD = errD_real + errD_fake\n","        # Update D\n","        optimizerD.step()\n","\n","        ############################\n","        # (2) Update G network: maximize log(D(G(z)))\n","        ###########################\n","        netG.zero_grad()\n","        label.fill_(real_label)  # fake labels are real for generator cost\n","        # Since we just updated D, perform another forward pass of all-fake batch through D\n","        output = netD(fake).view(-1)\n","        # Calculate G's loss based on this output\n","        errG = criterion(output, label)\n","        # Calculate gradients for G\n","        errG.backward()\n","        D_G_z2 = output.mean().item()\n","        # Update G\n","        optimizerG.step()\n","        images += batch_size\n","        # Output training stats\n","        if i % 10 == 0:\n","\n","            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n","                  % (epoch, num_epochs, i, len(dataloader),\n","                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n","\n","\n","        # Check how the generator is doing by saving G's output on fixed_noise\n","        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n","            with torch.no_grad():\n","                fake = netG(fixed_noise).detach().cpu()\n","            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n","        iters += 1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Starting Training Loop...\n","[0/400][0/24]\tLoss_D: 1.3911\tLoss_G: 0.6786\tD(x): 0.5428\tD(G(z)): 0.5414 / 0.5074\n","[0/400][10/24]\tLoss_D: 0.8638\tLoss_G: 1.0009\tD(x): 0.7351\tD(G(z)): 0.4230 / 0.3679\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-d83403eb120a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Calculate the gradients for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0merrD_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mD_G_z1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Add the gradients from the all-real and all-fake batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0merrD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrD_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0merrD_fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"tLhOBU5wme04"},"source":["**Plot generated images**"]},{"cell_type":"code","metadata":{"id":"OrX1kYxujwBP"},"source":["#%%capture\n","fig = plt.figure(figsize=(8,8))\n","plt.axis(\"off\")\n","ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n","ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n","\n","HTML(ani.to_jshtml())"],"execution_count":null,"outputs":[]}]}